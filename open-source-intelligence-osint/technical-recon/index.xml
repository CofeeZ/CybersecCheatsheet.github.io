<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Technical Recon on Offensive Security Cheatsheet</title>
    <link>/open-source-intelligence-osint/technical-recon/</link>
    <description>Recent content in Technical Recon on Offensive Security Cheatsheet</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    
	<atom:link href="/open-source-intelligence-osint/technical-recon/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Domain &amp; IP</title>
      <link>/open-source-intelligence-osint/technical-recon/domain_ip/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/open-source-intelligence-osint/technical-recon/domain_ip/</guid>
      <description>Identification Get information on an IP whois &amp;lt;IP&amp;gt; # Enhanced Whois https://domainbigdata.com https://whois.domaintools.com https://www.ip2location.com # Get IP address associated to a domain nslookup domain.fr ping domain.fr # Bing dorks to identify host sharing ip:xxx.xxx.xxx.xxx 
Online Passive Identification Tools # You can identify IP block owned by a company ipv4info.com # Then you can find domains and subdomains associated to an IP by using Passive Reverse DNS https://www.virustotal.com/gui/home/search RiskIQ Passive DNS # Many passive tests can be done http://www.</description>
    </item>
    
    <item>
      <title>Subdomain Enumeration</title>
      <link>/open-source-intelligence-osint/technical-recon/subdomain_enumeration/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/open-source-intelligence-osint/technical-recon/subdomain_enumeration/</guid>
      <description>Google Dorks &amp;amp; Bing Dorks # Google dorks are usefull for finding new subdomains site:wikipedia.org site:*.wikipedia.org -www -store -jobs -uk # Bing also support dorks and can give others results site: 
Online DNS tools and services # VirusTotal runs its own passive DNS replication service # DNS Dumpster can also find large number of sub-domains 
OWASP Amass # OWASP Amass tool suite is used to build a network map of the target # It relies for subdomain enumeration on scrapping data-sources, recursive bruteforcing, crawling web services, permuting names and reverse DNS sweeping # Basic use (DNS lookups and name alterations) amass -d example.</description>
    </item>
    
    <item>
      <title>Dorks</title>
      <link>/open-source-intelligence-osint/technical-recon/dorks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/open-source-intelligence-osint/technical-recon/dorks/</guid>
      <description>Google Dorks # Google Hacking made easy https://pentest-tools.com/information-gathering/google-hacking# # Search for documents on popular clouds site:drive.google.com &amp;lt;searchterm&amp;gt; site:dl.dropbox.com &amp;lt;searchterm&amp;gt; site:s3.amazonaws.com &amp;lt;searchterm&amp;gt; site:onedrive.live.com &amp;lt;searchterm&amp;gt; site:cryptome.org &amp;lt;searchterm&amp;gt; # Admins credentials intext:company_keyword &amp;amp; ext:txt | ext:sql | ext:cnf | ext:config | ext:log &amp;amp; intext:&amp;#34;admin&amp;#34; | intext:&amp;#34;root&amp;#34; | intext:&amp;#34;administrator&amp;#34; &amp;amp; intext:&amp;#34;password&amp;#34; | intext:&amp;#34;root&amp;#34; | intext:&amp;#34;admin&amp;#34; | intext:&amp;#34;administrator&amp;#34; # Look for domains indexed by others website site:bgp.he.net inurl:ndd site:dnslookup.fr inurl:ndd # Get information on the internal organization sites:cadres.</description>
    </item>
    
    <item>
      <title>Company General Informations</title>
      <link>/open-source-intelligence-osint/technical-recon/company_meta/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/open-source-intelligence-osint/technical-recon/company_meta/</guid>
      <description>Company Informations # Find informations about a company https://opencorporates.com # Find multiples sites owned by the same individual/company http://www.spyonweb.com https://www.crunchbase.com https://fr.kompass.com/ https://www.infogreffe.fr/ # Jobs https://www.glassdoor.fr/index.htm # Find people based on several websites https://recruitin.net/ # Cadastre FR https://cadastre.gouv.fr # Call for tender â†’ BOAMP https://www.boamp.fr/ # Legal and Financial informations (might not be free for full informations) https://www.societe.com/ # Brands https://www.inpi.fr/fr # Look for reported vulnerabilities https://www.</description>
    </item>
    
    <item>
      <title>Public Documents and Metadata</title>
      <link>/open-source-intelligence-osint/technical-recon/documents_metadata/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/open-source-intelligence-osint/technical-recon/documents_metadata/</guid>
      <description>Metagoofil # Extracting metadata of public documents (pdf,doc,xls,ppt,etc) availables in the target websites # The tool first perform a query in Google requesting different filetypes that can have useful metadata (pdf, doc, xls,ppt,etc) # Then will download those documents to the disk and extracts the metadata of the file using specific libraries for # parsing different file types (Hachoir, Pdfminer, etc) # Options # -d: domain to search # -t: filetype to download (pdf,doc,xls,ppt,odp,ods,docx,xlsx,pptx) # -l: limit of results to search (default 200) # -h: work with documents in directory (use \&amp;#34;yes\&amp;#34; for local analysis) # -n: limit of files to download # -o: working directory (location to save downloaded files) # -f: output file metagoofil.</description>
    </item>
    
    <item>
      <title>Photon</title>
      <link>/open-source-intelligence-osint/technical-recon/photon/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/open-source-intelligence-osint/technical-recon/photon/</guid>
      <description>General Informations # Photon is a fast OSINT web crawler which can retrieve the following data for a target : # URLs (in-scope &amp;amp; out-of-scope) # URLs with parameters (example.com/gallery.php?id=2) # Intel (emails, social media accounts, amazon buckets etc.) # Files (pdf, png, xml etc.) # Secret keys (auth/API keys &amp;amp; hashes) # JavaScript files &amp;amp; Endpoints present in them # Strings matching custom regex pattern # Subdomains &amp;amp; DNS related data # You can use the --ninja 4 option to get 4 clients requesting server.</description>
    </item>
    
    <item>
      <title>MISC</title>
      <link>/open-source-intelligence-osint/technical-recon/misc/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/open-source-intelligence-osint/technical-recon/misc/</guid>
      <description>Orbit - Cryto Wallet Analysis # Give it a blockchain based crypto wallet address and it will crawl 3 levels deep in transaction data to plot a graph out of the information. python3 orbit.py # Next thing is to plot a graph for which we will be using quark framework # Then use the html output python quark.py /path/to/file.json 
Wayback Machine (https://archive.org/web/) # You can also uses as CLI tool https://github.</description>
    </item>
    
  </channel>
</rss>